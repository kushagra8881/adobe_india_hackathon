# ğŸ“„ Document Intelligence System

A powerful document intelligence system that extracts and prioritizes the most relevant sections from multiple documents based on a specific persona and their job-to-be-done. The system uses advanced AI models to understand document content and provide intelligent analysis.

## ğŸš€ Features

### ğŸ§  **Intelligent Analysis**
- **Semantic Understanding**: Uses sentence transformers for deep content analysis
- **Persona-Based Processing**: Tailors results to specific user needs and jobs
- **Relevance Ranking**: Intelligently ranks document sections by importance
- **Context-Aware Extraction**: Maintains document structure and relationships

### ğŸ› ï¸ **Advanced Processing**
- **PDF Processing**: Handles complex PDF documents with text extraction
- **OCR Support**: Processes image-based content when needed
- **Local Model Caching**: Downloads and caches AI models for offline use
- **Robust Error Handling**: Comprehensive error recovery and validation

### ğŸ“Š **Performance Features**
- **Fast Processing**: Optimized for quick document analysis
- **Memory Efficient**: Smart model loading and resource management
- **Comprehensive Logging**: Detailed processing information and timing
- **Flexible Output**: JSON format with detailed metadata

## ğŸ“‹ System Requirements

### Minimum Requirements
- **Python**: 3.7+ (64-bit recommended)
- **Memory**: 4GB RAM (8GB+ recommended)
- **Storage**: 2GB free space for models and cache
- **Optional**: CUDA-compatible GPU for acceleration

## ğŸ› ï¸ Installation & Setup

### Quick Setup
```bash
# Install dependencies and setup models
python setup.py

# Run the system
python run.py --input-dir "./documents" --persona "travel planner" --job "plan trip"
```

### Manual Installation
```bash
# Install Python dependencies
pip install -r requirements.txt

# System dependencies (Ubuntu/Debian)
sudo apt-get update
sudo apt-get install tesseract-ocr libtesseract-dev poppler-utils

# Verify installation
python run.py --help
```

### Docker Deployment
```bash
# Build and run
docker build -t doc-intelligence .
docker run -v ./documents:/app/documents doc-intelligence
```

## ğŸ¯ Usage

### Basic Usage
```bash
python run.py \
    --input-dir "./Collection 1/PDFs" \
    --persona "Travel Planner" \
    --job "Plan a trip of 4 days for a group of 10 college friends"
```

### Advanced Options
```bash
python run.py \
    --input-dir "./documents" \
    --persona "Corporate Trainer" \
    --job "Develop Adobe Acrobat training for new employees" \
    --output-file "training_analysis.json" \
    --top-sections 10 \
    --top-subsections 8 \
    --use-ocr \
    --verbose
```

### Command Line Options
- `--input-dir`: Directory containing PDF files (required)
- `--persona`: Description of the user persona (required)
- `--job`: Job to be done by the persona (required)
- `--output-file`: Path to output JSON file (default: challenge1b_output.json)
- `--top-sections`: Number of top sections to extract (default: 5)
- `--top-subsections`: Number of top subsections to analyze (default: 5)
- `--models-dir`: Directory to store downloaded models (default: ./models)
- `--use-ocr`: Enable OCR for image-based text extraction
- `--verbose`: Enable verbose logging

## ğŸ§ª Testing

### Run Tests
```bash
# Basic functionality test
python test.py

# Setup verification
python setup.py  # Will also verify the installation
```

## ğŸ“ Project Structure

```
Challenge_1b/
â”œâ”€â”€ ğŸ§  Core Components
â”‚   â”œâ”€â”€ model_manager.py          # AI model management
â”‚   â”œâ”€â”€ document_processor.py     # PDF processing
â”‚   â”œâ”€â”€ relevance_ranker.py       # Section ranking
â”‚   â”œâ”€â”€ subsection_analyzer.py    # Text analysis
â”‚   â””â”€â”€ run.py                    # Main execution script
â”œâ”€â”€ ğŸ› ï¸ Setup & Configuration
â”‚   â”œâ”€â”€ setup.py                  # Installation and setup
â”‚   â”œâ”€â”€ requirements.txt          # Dependencies
â”‚   â”œâ”€â”€ Dockerfile               # Container configuration
â”‚   â””â”€â”€ test.py                  # Basic testing
â”œâ”€â”€ ğŸ“ Data Collections
â”‚   â”œâ”€â”€ Collection 1/            # South of France travel docs
â”‚   â”œâ”€â”€ Collection 2/            # Adobe Acrobat training docs
â”‚   â””â”€â”€ documents/               # User documents
â””â”€â”€ ğŸ“„ Documentation
    â”œâ”€â”€ README.md                # This guide
    â””â”€â”€ approach_explanation.md  # Technical methodology
```

## ğŸ¯ Example Scenarios

### Travel Planning
```bash
python run.py \
    --input-dir "./Collection 1/PDFs" \
    --persona "Travel Planner" \
    --job "Plan a trip of 4 days for a group of 10 college friends"
```

### Corporate Training
```bash
python run.py \
    --input-dir "./Collection 2/PDFs" \
    --persona "Corporate Trainer" \
    --job "Develop Adobe Acrobat training for new employees"
```

### Research Analysis
```bash
python run.py \
    --input-dir "./documents" \
    --persona "Research Analyst" \
    --job "Extract key insights for executive summary"
```

## ğŸ“Š Output Format

The system generates a JSON file with the following structure:

```json
{
    "metadata": {
        "input_documents": ["doc1.pdf", "doc2.pdf"],
        "persona": "Travel Planner",
        "job_to_be_done": "Plan a trip...",
        "processing_timestamp": "2025-01-01T12:00:00",
        "processing_time_seconds": 45.67,
        "system_info": {
            "models_used": {"sentence_transformer": "locally_cached"},
            "documents_processed": 7,
            "total_sections_found": 42
        }
    },
    "extracted_sections": [
        {
            "document": "doc1.pdf",
            "section_title": "Budget Travel Tips",
            "importance_rank": 1,
            "page_number": 3,
            "relevance_score": 0.89
        }
    ],
    "subsection_analysis": [
        {
            "document": "doc1.pdf",
            "refined_text": "Key insights about budget travel...",
            "page_number": 3,
            "text_length": 245
        }
    ]
}
```

## ğŸ”§ Performance Features

### Model Management
- **Local Caching**: AI models downloaded once and reused
- **Automatic Setup**: Models downloaded automatically on first run
- **GPU Support**: Automatic CUDA detection and usage when available
- **Memory Optimization**: Efficient model loading and resource management

### Processing Capabilities
- **Document Types**: PDF files with text and image content
- **OCR Integration**: Automatic fallback to OCR for image-based text
- **Scale**: Handles single documents to large collections
- **Speed**: Optimized processing pipelines for quick results

## ğŸ› Troubleshooting

### Common Issues & Solutions

#### Model Download Issues
```bash
# Re-run setup to download models
python setup.py

# Check internet connectivity and storage space
```

#### PDF Processing Issues
```bash
# Try with OCR enabled
python run.py --input-dir "./documents" --persona "..." --job "..." --use-ocr

# Check PDF file permissions and corruption
```

#### Memory Issues
```bash
# Process fewer documents at once
# Ensure sufficient RAM (4GB+ recommended)
```

## ğŸš€ Performance Benchmarks

### Processing Speed
- **Small Collection** (5-10 PDFs): 30-60 seconds
- **Medium Collection** (10-20 PDFs): 1-3 minutes
- **Large Collection** (20+ PDFs): 3-10 minutes

### Accuracy
- **Relevance Ranking**: High precision with semantic analysis
- **Text Extraction**: 95%+ accuracy with OCR fallback
- **Context Preservation**: Maintains document structure and relationships

## ğŸ¤ Contributing

### Development Setup
```bash
# Clone and setup
cd Challenge_1b
python setup.py

# Run tests
python test.py
```

## ğŸ“„ License & Credits

### System Credits
- **AI Models**: Sentence Transformers for semantic analysis
- **PDF Processing**: PyMuPDF for document handling
- **OCR**: Tesseract for image-based text extraction
- **ML Framework**: PyTorch for model execution

### Dependencies
- Sentence Transformers: Apache 2.0
- PyMuPDF: AGPL-3.0
- PyTorch: BSD-3-Clause
- NLTK: Apache 2.0

---

*Document Intelligence System - Intelligent document analysis powered by AI* ğŸ“„ğŸ§ 
