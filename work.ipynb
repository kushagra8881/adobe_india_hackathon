{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91b2f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import csv\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# # Path to JSON files\n",
    "# json_files = glob.glob('/home/kushagra/Documents/code/AI/Adobe-India-Hackathon25-main/adobe_india_hackathon/Challenge - 1(a)/Datasets/Output.json/*.json')\n",
    "\n",
    "# # Output CSV file\n",
    "# csv_filename = 'output_outline.csv'\n",
    "# fieldnames = ['file', 'title', 'level', 'text', 'page']  # Added 'title'\n",
    "\n",
    "# with open(csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "    \n",
    "#     for json_file in json_files:\n",
    "#         with open(json_file, 'r', encoding='utf-8') as f:\n",
    "#             try:\n",
    "#                 data = json.load(f)\n",
    "#                 title = data.get('title', '')  # Get title once per file\n",
    "#                 for item in data.get('outline', []):\n",
    "#                     writer.writerow({\n",
    "#                         'file': os.path.basename(json_file),\n",
    "#                         'title': title,\n",
    "#                         'level': item.get('level', ''),\n",
    "#                         'text': item.get('text', ''),\n",
    "#                         'page': item.get('page', '')\n",
    "#                     })\n",
    "#             except json.JSONDecodeError as e:\n",
    "#                 print(f\"Error decoding JSON file {json_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cfe6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('output_outline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22248aae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>title</th>\n",
       "      <th>level</th>\n",
       "      <th>text</th>\n",
       "      <th>page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E0CCG5S312.json</td>\n",
       "      <td>Overview  Foundation Level Extensions</td>\n",
       "      <td>H1</td>\n",
       "      <td>Revision History</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E0CCG5S312.json</td>\n",
       "      <td>Overview  Foundation Level Extensions</td>\n",
       "      <td>H1</td>\n",
       "      <td>Table of Contents</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E0CCG5S312.json</td>\n",
       "      <td>Overview  Foundation Level Extensions</td>\n",
       "      <td>H1</td>\n",
       "      <td>Acknowledgements</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E0CCG5S312.json</td>\n",
       "      <td>Overview  Foundation Level Extensions</td>\n",
       "      <td>H1</td>\n",
       "      <td>1. Introduction to the Foundation Level Extens...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E0CCG5S312.json</td>\n",
       "      <td>Overview  Foundation Level Extensions</td>\n",
       "      <td>H1</td>\n",
       "      <td>2. Introduction to Foundation Level Agile Test...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>STEMPathwaysFlyer.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>H1</td>\n",
       "      <td>Parsippany -Troy Hills STEM Pathways</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>STEMPathwaysFlyer.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>H2</td>\n",
       "      <td>PATHWAY OPTIONS</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>STEMPathwaysFlyer.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>H2</td>\n",
       "      <td>Elective Course Offerings</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>STEMPathwaysFlyer.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>H3</td>\n",
       "      <td>What Colleges Say!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>TOPJUMP-PARTY-INVITATION-20161003-V01.json</td>\n",
       "      <td>NaN</td>\n",
       "      <td>H1</td>\n",
       "      <td>HOPE To SEE You THERE!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          file  \\\n",
       "0                              E0CCG5S312.json   \n",
       "1                              E0CCG5S312.json   \n",
       "2                              E0CCG5S312.json   \n",
       "3                              E0CCG5S312.json   \n",
       "4                              E0CCG5S312.json   \n",
       "..                                         ...   \n",
       "56                      STEMPathwaysFlyer.json   \n",
       "57                      STEMPathwaysFlyer.json   \n",
       "58                      STEMPathwaysFlyer.json   \n",
       "59                      STEMPathwaysFlyer.json   \n",
       "60  TOPJUMP-PARTY-INVITATION-20161003-V01.json   \n",
       "\n",
       "                                      title level  \\\n",
       "0   Overview  Foundation Level Extensions      H1   \n",
       "1   Overview  Foundation Level Extensions      H1   \n",
       "2   Overview  Foundation Level Extensions      H1   \n",
       "3   Overview  Foundation Level Extensions      H1   \n",
       "4   Overview  Foundation Level Extensions      H1   \n",
       "..                                      ...   ...   \n",
       "56                                      NaN    H1   \n",
       "57                                      NaN    H2   \n",
       "58                                      NaN    H2   \n",
       "59                                      NaN    H3   \n",
       "60                                      NaN    H1   \n",
       "\n",
       "                                                 text  page  \n",
       "0                                   Revision History      2  \n",
       "1                                  Table of Contents      3  \n",
       "2                                   Acknowledgements      4  \n",
       "3   1. Introduction to the Foundation Level Extens...     5  \n",
       "4   2. Introduction to Foundation Level Agile Test...     6  \n",
       "..                                                ...   ...  \n",
       "56               Parsippany -Troy Hills STEM Pathways     0  \n",
       "57                                    PATHWAY OPTIONS     0  \n",
       "58                          Elective Course Offerings     1  \n",
       "59                                 What Colleges Say!     1  \n",
       "60                            HOPE To SEE You THERE!      0  \n",
       "\n",
       "[61 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db13ad49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (61, 5)\n",
      "\n",
      "Column names: ['file', 'title', 'level', 'text', 'page']\n",
      "\n",
      "Unique levels: ['H1' 'H2' 'H3' 'H4']\n",
      "\n",
      "Level counts:\n",
      "level\n",
      "H3    26\n",
      "H2    20\n",
      "H1    11\n",
      "H4     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "First few rows:\n",
      "              file                                    title level  \\\n",
      "0  E0CCG5S312.json  Overview  Foundation Level Extensions      H1   \n",
      "1  E0CCG5S312.json  Overview  Foundation Level Extensions      H1   \n",
      "2  E0CCG5S312.json  Overview  Foundation Level Extensions      H1   \n",
      "3  E0CCG5S312.json  Overview  Foundation Level Extensions      H1   \n",
      "4  E0CCG5S312.json  Overview  Foundation Level Extensions      H1   \n",
      "\n",
      "                                                text  page  \n",
      "0                                  Revision History      2  \n",
      "1                                 Table of Contents      3  \n",
      "2                                  Acknowledgements      4  \n",
      "3  1. Introduction to the Foundation Level Extens...     5  \n",
      "4  2. Introduction to Foundation Level Agile Test...     6  \n"
     ]
    }
   ],
   "source": [
    "# Check data structure and unique levels\n",
    "print(\"Data shape:\", data.shape)\n",
    "print(\"\\nColumn names:\", data.columns.tolist())\n",
    "print(\"\\nUnique levels:\", data['level'].unique())\n",
    "print(\"\\nLevel counts:\")\n",
    "print(data['level'].value_counts())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99561f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08770719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for Random Forest classification...\n",
      "Cleaned data shape: (56, 6)\n",
      "Level distribution after cleaning:\n",
      "level\n",
      "H3    25\n",
      "H2    18\n",
      "H1     9\n",
      "H4     4\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2672572/1276247304.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_clean['combined_text'] = data_clean['title'].astype(str) + ' ' + data_clean['text'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing and feature engineering\n",
    "print(\"Preparing data for Random Forest classification...\")\n",
    "\n",
    "# Clean and prepare the data\n",
    "data_clean = data.dropna(subset=['title', 'text', 'level'])\n",
    "\n",
    "# Combine title and text for feature extraction\n",
    "data_clean['combined_text'] = data_clean['title'].astype(str) + ' ' + data_clean['text'].astype(str)\n",
    "\n",
    "# Remove empty or very short texts\n",
    "data_clean = data_clean[data_clean['combined_text'].str.len() > 3]\n",
    "\n",
    "print(f\"Cleaned data shape: {data_clean.shape}\")\n",
    "print(f\"Level distribution after cleaning:\")\n",
    "print(data_clean['level'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cf11171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features using TF-IDF with multilingual support...\n",
      "Feature matrix shape: (56, 954)\n",
      "Target variable shape: (56,)\n",
      "Vectorizer encoding: utf-8\n",
      "Vectorizer analyzer: char_wb\n",
      "Training set size: 50\n",
      "Testing set size: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushagra/Documents/code/AI/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:555: UserWarning: The parameter 'token_pattern' will not be used since 'analyzer' != 'word'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction using TF-IDF with multilingual UTF-8 support\n",
    "print(\"Extracting features using TF-IDF with multilingual support...\")\n",
    "\n",
    "# Create TF-IDF features from combined text with UTF-8 and multilingual support\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,  # Increased for multilingual content\n",
    "    ngram_range=(1, 3),  # Use unigrams, bigrams, and trigrams for better multilingual support\n",
    "    min_df=1,  # Reduced to handle diverse languages\n",
    "    max_df=0.9,  # Slightly increased threshold\n",
    "    lowercase=True,  # Convert to lowercase for consistency\n",
    "    analyzer='char_wb',  # Character-based analysis for multilingual support\n",
    "    encoding='utf-8',  # Explicit UTF-8 encoding\n",
    "    decode_error='ignore',  # Handle encoding errors gracefully\n",
    "    strip_accents='unicode',  # Handle accented characters\n",
    "    token_pattern=r'(?u)\\b\\w+\\b'  # Unicode-aware word boundaries\n",
    ")\n",
    "\n",
    "# Alternative configuration for word-based analysis (uncomment if preferred)\n",
    "# vectorizer = TfidfVectorizer(\n",
    "#     max_features=2000,\n",
    "#     stop_words=None,  # No stop words for multilingual support\n",
    "#     ngram_range=(1, 2),\n",
    "#     min_df=1,\n",
    "#     max_df=0.9,\n",
    "#     lowercase=True,\n",
    "#     encoding='utf-8',\n",
    "#     decode_error='ignore',\n",
    "#     strip_accents='unicode'\n",
    "# )\n",
    "\n",
    "# Fit and transform the text data\n",
    "X = vectorizer.fit_transform(data_clean['combined_text'])\n",
    "y = data_clean['level']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable shape: {y.shape}\")\n",
    "print(f\"Vectorizer encoding: {vectorizer.encoding}\")\n",
    "print(f\"Vectorizer analyzer: {vectorizer.analyzer}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Testing set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c483f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Random Forest Classifier with GridSearch and Class Weighting...\n",
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "\n",
      "Best Parameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Model Accuracy: 0.8333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          H1       1.00      1.00      1.00         1\n",
      "          H2       1.00      0.50      0.67         2\n",
      "          H3       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.92      0.83      0.84         6\n",
      "weighted avg       0.88      0.83      0.82         6\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 0 0 0]\n",
      " [0 1 1 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 0]]\n",
      "\n",
      "Best Parameters found by GridSearchCV:\n",
      "{'bootstrap': True, 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\n",
      "Model Accuracy: 0.8333\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          H1       1.00      1.00      1.00         1\n",
      "          H2       1.00      0.50      0.67         2\n",
      "          H3       0.75      1.00      0.86         3\n",
      "\n",
      "    accuracy                           0.83         6\n",
      "   macro avg       0.92      0.83      0.84         6\n",
      "weighted avg       0.88      0.83      0.82         6\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1 0 0 0]\n",
      " [0 1 1 0]\n",
      " [0 0 3 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Train Random Forest Classifier with hyperparameter tuning and class weighting\n",
    "print(\"Training Random Forest Classifier with GridSearch and Class Weighting...\")\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Create a Random Forest classifier with balanced class weights\n",
    "rf_classifier_balanced = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    class_weight='balanced'  # Address data imbalance\n",
    ")\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_classifier_balanced,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "rf_classifier = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\nBest Parameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Make predictions with the best model\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab917003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the prediction function:\n",
      "--------------------------------------------------\n",
      "Title: 'Introduction'\n",
      "Text: 'This document provides an overview of the foundation level extensions'\n",
      "Predicted Level: H2\n",
      "Confidence Scores: {'H1': 0.23984548784548784, 'H2': 0.6984795983743354, 'H3': 0.061674913780176935, 'H4': 0.0}\n",
      "\n",
      "Title: 'Testing Methods'\n",
      "Text: 'Various testing approaches and methodologies used in software development'\n",
      "Predicted Level: H2\n",
      "Confidence Scores: {'H1': 0.2567142857142858, 'H2': 0.5789341269288432, 'H3': 0.13435158735687142, 'H4': 0.03}\n",
      "\n",
      "Title: 'Specific Implementation Details'\n",
      "Text: 'Detailed explanation of implementation steps and procedures'\n",
      "Predicted Level: H2\n",
      "Confidence Scores: {'H1': 0.320418747918748, 'H2': 0.5875345434292805, 'H3': 0.08204670865197179, 'H4': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Function to predict heading level for new title and text\n",
    "def predict_heading_level(title, text):\n",
    "    \"\"\"\n",
    "    Predict the heading level (H1, H2, H3, H4) for given title and text\n",
    "    \n",
    "    Args:\n",
    "        title (str): The title/heading text\n",
    "        text (str): The content text\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predicted_level, confidence_scores)\n",
    "    \"\"\"\n",
    "    # Combine title and text\n",
    "    combined_text = str(title) + ' ' + str(text)\n",
    "    \n",
    "    # Transform using the same vectorizer\n",
    "    text_features = vectorizer.transform([combined_text])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = rf_classifier.predict(text_features)[0]\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    probabilities = rf_classifier.predict_proba(text_features)[0]\n",
    "    classes = rf_classifier.classes_\n",
    "    \n",
    "    # Create confidence scores dictionary\n",
    "    confidence_scores = dict(zip(classes, probabilities))\n",
    "    \n",
    "    return prediction, confidence_scores\n",
    "\n",
    "# Test the prediction function with some examples\n",
    "print(\"Testing the prediction function:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Example 1\n",
    "title1 = \"Introduction\"\n",
    "text1 = \"This document provides an overview of the foundation level extensions\"\n",
    "pred1, conf1 = predict_heading_level(title1, text1)\n",
    "print(f\"Title: '{title1}'\")\n",
    "print(f\"Text: '{text1}'\")\n",
    "print(f\"Predicted Level: {pred1}\")\n",
    "print(f\"Confidence Scores: {conf1}\")\n",
    "print()\n",
    "\n",
    "# Example 2\n",
    "title2 = \"Testing Methods\"\n",
    "text2 = \"Various testing approaches and methodologies used in software development\"\n",
    "pred2, conf2 = predict_heading_level(title2, text2)\n",
    "print(f\"Title: '{title2}'\")\n",
    "print(f\"Text: '{text2}'\")\n",
    "print(f\"Predicted Level: {pred2}\")\n",
    "print(f\"Confidence Scores: {conf2}\")\n",
    "print()\n",
    "\n",
    "# Example 3\n",
    "title3 = \"Specific Implementation Details\"\n",
    "text3 = \"Detailed explanation of implementation steps and procedures\"\n",
    "pred3, conf3 = predict_heading_level(title3, text3)\n",
    "print(f\"Title: '{title3}'\")\n",
    "print(f\"Text: '{text3}'\")\n",
    "print(f\"Predicted Level: {pred3}\")\n",
    "print(f\"Confidence Scores: {conf3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e51524b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing multilingual capabilities:\n",
      "============================================================\n",
      "ENGLISH EXAMPLES:\n",
      "------------------------------\n",
      "EN Title: 'Chapter Introduction'\n",
      "EN Text: 'This chapter covers the basic concepts'\n",
      "Predicted: H2, Confidence: 0.670\n",
      "\n",
      "SPANISH EXAMPLES:\n",
      "------------------------------\n",
      "ES Title: 'Introducción al Capítulo'\n",
      "ES Text: 'Este capítulo cubre los conceptos básicos'\n",
      "Predicted: H2, Confidence: 0.666\n",
      "\n",
      "FRENCH EXAMPLES:\n",
      "------------------------------\n",
      "FR Title: 'Introduction du Chapitre'\n",
      "FR Text: 'Ce chapitre couvre les concepts de base'\n",
      "Predicted: H2, Confidence: 0.683\n",
      "\n",
      "GERMAN EXAMPLES:\n",
      "------------------------------\n",
      "DE Title: 'Kapitel Einführung'\n",
      "DE Text: 'Dieses Kapitel behandelt die Grundkonzepte'\n",
      "Predicted: H2, Confidence: 0.652\n",
      "\n",
      "ACCENTED CHARACTERS:\n",
      "------------------------------\n",
      "Accented Title: 'Configuración Avanzada'\n",
      "Accented Text: 'Configuración detallada de parámetros específicos'\n",
      "Predicted: H2, Confidence: 0.637\n",
      "\n",
      "TEST:\n",
      "------------------------------\n",
      "Accented Title: ''\n",
      "Accented Text: 'What Colleges Say!'\n",
      "Predicted: H2, Confidence: 0.664\n",
      "\n",
      "MIXED LANGUAGE:\n",
      "------------------------------\n",
      "Mixed Title: 'API Documentation'\n",
      "Mixed Text: 'Documentation complète pour l'API REST'\n",
      "Predicted: H2, Confidence: 0.642\n",
      "\n",
      "JAPANESE EXAMPLES:\n",
      "------------------------------\n",
      "JP Title: '章の紹介'\n",
      "JP Text: 'この章では基本的な概念について説明します'\n",
      "Predicted: H2, Confidence: 0.711\n"
     ]
    }
   ],
   "source": [
    "# Test multilingual capabilities\n",
    "print(\"Testing multilingual capabilities:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# English examples\n",
    "print(\"ENGLISH EXAMPLES:\")\n",
    "print(\"-\" * 30)\n",
    "pred_en1, conf_en1 = predict_heading_level(\"Chapter Introduction\", \"This chapter covers the basic concepts\")\n",
    "print(f\"EN Title: 'Chapter Introduction'\")\n",
    "print(f\"EN Text: 'This chapter covers the basic concepts'\")\n",
    "print(f\"Predicted: {pred_en1}, Confidence: {max(conf_en1.values()):.3f}\")\n",
    "print()\n",
    "\n",
    "# Spanish examples\n",
    "print(\"SPANISH EXAMPLES:\")\n",
    "print(\"-\" * 30)\n",
    "pred_es1, conf_es1 = predict_heading_level(\"Introducción al Capítulo\", \"Este capítulo cubre los conceptos básicos\")\n",
    "print(f\"ES Title: 'Introducción al Capítulo'\")\n",
    "print(f\"ES Text: 'Este capítulo cubre los conceptos básicos'\")\n",
    "print(f\"Predicted: {pred_es1}, Confidence: {max(conf_es1.values()):.3f}\")\n",
    "print()\n",
    "\n",
    "# French examples\n",
    "print(\"FRENCH EXAMPLES:\")\n",
    "print(\"-\" * 30)\n",
    "pred_fr1, conf_fr1 = predict_heading_level(\"Introduction du Chapitre\", \"Ce chapitre couvre les concepts de base\")\n",
    "print(f\"FR Title: 'Introduction du Chapitre'\")\n",
    "print(f\"FR Text: 'Ce chapitre couvre les concepts de base'\")\n",
    "print(f\"Predicted: {pred_fr1}, Confidence: {max(conf_fr1.values()):.3f}\")\n",
    "print()\n",
    "\n",
    "# German examples\n",
    "print(\"GERMAN EXAMPLES:\")\n",
    "print(\"-\" * 30)\n",
    "pred_de1, conf_de1 = predict_heading_level(\"Kapitel Einführung\", \"Dieses Kapitel behandelt die Grundkonzepte\")\n",
    "print(f\"DE Title: 'Kapitel Einführung'\")\n",
    "print(f\"DE Text: 'Dieses Kapitel behandelt die Grundkonzepte'\")\n",
    "print(f\"Predicted: {pred_de1}, Confidence: {max(conf_de1.values()):.3f}\")\n",
    "print()\n",
    "\n",
    "# Test with accented characters\n",
    "print(\"ACCENTED CHARACTERS:\")\n",
    "print(\"-\" * 30)\n",
    "pred_acc, conf_acc = predict_heading_level(\"Configuración Avanzada\", \"Configuración detallada de parámetros específicos\")\n",
    "print(f\"Accented Title: 'Configuración Avanzada'\")\n",
    "print(f\"Accented Text: 'Configuración detallada de parámetros específicos'\")\n",
    "print(f\"Predicted: {pred_acc}, Confidence: {max(conf_acc.values()):.3f}\")\n",
    "print()\n",
    "\n",
    "print(\"TEST:\")\n",
    "print(\"-\" * 30)\n",
    "pred_acc2, conf_acc2 = predict_heading_level(\" \", \"What Colleges Say!\")\n",
    "print(f\"Accented Title: ''\")\n",
    "print(f\"Accented Text: 'What Colleges Say!'\")\n",
    "print(f\"Predicted: {pred_acc2}, Confidence: {max(conf_acc2.values()):.3f}\")\n",
    "print()\n",
    "\n",
    "# Test with mixed languages\n",
    "print(\"MIXED LANGUAGE:\")\n",
    "print(\"-\" * 30)\n",
    "pred_mix, conf_mix = predict_heading_level(\"API Documentation\", \"Documentation complète pour l'API REST\")\n",
    "print(f\"Mixed Title: 'API Documentation'\")\n",
    "print(f\"Mixed Text: 'Documentation complète pour l'API REST'\")\n",
    "print(f\"Predicted: {pred_mix}, Confidence: {max(conf_mix.values()):.3f}\")\n",
    "print()\n",
    "\n",
    "# Japanese examples\n",
    "print(\"JAPANESE EXAMPLES:\")\n",
    "print(\"-\" * 30)\n",
    "pred_jp, conf_jp = predict_heading_level(\"章の紹介\", \"この章では基本的な概念について説明します\")\n",
    "print(f\"JP Title: '章の紹介'\")\n",
    "print(f\"JP Text: 'この章では基本的な概念について説明します'\")\n",
    "print(f\"Predicted: {pred_jp}, Confidence: {max(conf_jp.values()):.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
